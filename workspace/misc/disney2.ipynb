{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danpasse/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/danpasse/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat_id': 391,\n",
       " 'job_id': 19757793040,\n",
       " 'step_1': ['* You are passionate about your area of expertise, deeply inquisitive and open minded, informed, but not limited, by your domain of expertise.\\n* You enjoy intellectual debate.\\n* You are comfortable guiding other team members but willing to get your hands dirty and help build research systems when needed\\n* You are driven to perform research that is not simply novel, but deeply impactful to the company and society have a deep desire to reduce the research to practice in the form of prototypes and technology demonstrators.\\n* You are excited by the platform provided by Disney to connect with children and guests of all ages to have a positive impact on the world.\\n* You have a deep sensitivity for ethical use of technology and data.\\n* You thrive in a fast-paced collaborative environment.\\n* You are self-directed and independent, but open to constructive feedback.\\n* You are a team player, able to work in collaborative interdisciplinary groups\\n* You are willing to take the time necessary to understand the business and adapt your knowledge to maximize the value of your contributions.\\n* You understand that communicating the value of your research to non-technical stakeholders is critical to continued investment.\\n* You understand that there are times when deep scientific rigor is necessary to validate a concept, and times when this is counterproductive, and are eager to discern the difference.\\n* You value sharing knowledge with the world but understand the need to drive and retain value in a private company.']}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "with open('../../data/disney/job_descriptions.json', 'r') as reader:\n",
    "  for description in json.loads(reader.read()):\n",
    "    items = [\n",
    "      item['text']\n",
    "      for item in description['sections']\n",
    "      if item['section'] == 'Basic Qualifications' or item['section'] == 'Preferred Qualifications'\n",
    "    ]\n",
    "\n",
    "    if len(items) == 0:\n",
    "      continue\n",
    "\n",
    "    qualification = {\n",
    "      'cat_id': description['cat_id'],\n",
    "      'job_id': description['job_id'],\n",
    "      'step_1': items\n",
    "    }\n",
    "\n",
    "    documents.append(qualification)\n",
    "\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* You are passionate about your area of expertise, deeply inquisitive and open minded, informed, but not limited, by your domain of expertise.\n",
      "* You enjoy intellectual debate.\n",
      "* You are comfortable guiding other team members but willing to get your hands dirty and help build research systems when needed\n",
      "* You are driven to perform research that is not simply novel, but deeply impactful to the company and society have a deep desire to reduce the research to practice in the form of prototypes and technology demonstrators.\n",
      "* You are excited by the platform provided by Disney to connect with children and guests of all ages to have a positive impact on the world.\n",
      "* You have a deep sensitivity for ethical use of technology and data.\n",
      "* You thrive in a fast-paced collaborative environment.\n",
      "* You are self-directed and independent, but open to constructive feedback.\n",
      "* You are a team player, able to work in collaborative interdisciplinary groups\n",
      "* You are willing to take the time necessary to understand the business and adapt your knowledge to maximize the value of your contributions.\n",
      "* You understand that communicating the value of your research to non-technical stakeholders is critical to continued investment.\n",
      "* You understand that there are times when deep scientific rigor is necessary to validate a concept, and times when this is counterproductive, and are eager to discern the difference.\n",
      "* You value sharing knowledge with the world but understand the need to drive and retain value in a private company.\n",
      "\n",
      "you are passionate about your area of expertise deeply inquisitive and open minded informed but not limited by your domain of expertise\n"
     ]
    }
   ],
   "source": [
    "def transform_text(text):\n",
    "  def clean_sentence(sentence):\n",
    "    sentence = re.sub(r'[`\\']', '', sentence)\n",
    "    sentence = re.sub(r'[,.!?:;\"]', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence.strip()\n",
    "\n",
    "  text = re.sub(r'(e\\.g\\.|\\/)', ' ', text)\n",
    "  text = re.sub(r'etc\\.', 'etc', text)\n",
    "  text = re.sub(r'[*•\"()]', ' ', text)\n",
    "  text = re.sub(r'&', ' and ', text)\n",
    "\n",
    "  return [ clean_sentence(sentence) for sentence in sent_tokenize(text.lower()) ]\n",
    "\n",
    "for document in documents:\n",
    "  document['step_2'] = []\n",
    "  for section_text in document['step_1']:\n",
    "    document['step_2'].extend(transform_text(section_text))\n",
    "\n",
    "print(documents[0]['step_1'][0])\n",
    "print()\n",
    "print(documents[0]['step_2'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are passionate about your area of expertise deeply inquisitive and open minded informed but not limited by your domain of expertise\n",
      "\n",
      "['you', 'are', 'passionate', 'about', 'your', 'area', 'of', 'expertise', 'deeply', 'inquisitive', 'and', 'open', 'minded', 'informed', 'but', 'not', 'limited', 'by', 'your', 'domain', 'of', 'expertise']\n"
     ]
    }
   ],
   "source": [
    "for document in documents:\n",
    "  document['step_3'] = []\n",
    "  for sentence in document['step_2']:\n",
    "    document['step_3'].append(word_tokenize(sentence))\n",
    "  \n",
    "print(documents[0]['step_2'][0])\n",
    "print()\n",
    "print(documents[0]['step_3'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'are', 'passionate', 'about', 'your', 'area', 'of', 'expertise', 'deeply', 'inquisitive', 'and', 'open', 'minded', 'informed', 'but', 'not', 'limited', 'by', 'your', 'domain', 'of', 'expertise']\n",
      "\n",
      "['you_are', 'are_passionate', 'passionate_about', 'about_your', 'your_area', 'area_of', 'of_expertise', 'expertise_deeply', 'deeply_inquisitive', 'inquisitive_and', 'and_open', 'open_minded', 'minded_informed', 'informed_but', 'but_not', 'not_limited', 'limited_by', 'by_your', 'your_domain', 'domain_of', 'of_expertise', 'you_are_passionate', 'are_passionate_about', 'passionate_about_your', 'about_your_area', 'your_area_of', 'area_of_expertise', 'of_expertise_deeply', 'expertise_deeply_inquisitive', 'deeply_inquisitive_and', 'inquisitive_and_open', 'and_open_minded', 'open_minded_informed', 'minded_informed_but', 'informed_but_not', 'but_not_limited', 'not_limited_by', 'limited_by_your', 'by_your_domain', 'your_domain_of', 'domain_of_expertise', 'you_are_passionate_about', 'are_passionate_about_your', 'passionate_about_your_area', 'about_your_area_of', 'your_area_of_expertise', 'area_of_expertise_deeply', 'of_expertise_deeply_inquisitive', 'expertise_deeply_inquisitive_and', 'deeply_inquisitive_and_open', 'inquisitive_and_open_minded', 'and_open_minded_informed', 'open_minded_informed_but', 'minded_informed_but_not', 'informed_but_not_limited', 'but_not_limited_by', 'not_limited_by_your', 'limited_by_your_domain', 'by_your_domain_of', 'your_domain_of_expertise', 'you_are_passionate_about_your', 'are_passionate_about_your_area', 'passionate_about_your_area_of', 'about_your_area_of_expertise', 'your_area_of_expertise_deeply', 'area_of_expertise_deeply_inquisitive', 'of_expertise_deeply_inquisitive_and', 'expertise_deeply_inquisitive_and_open', 'deeply_inquisitive_and_open_minded', 'inquisitive_and_open_minded_informed', 'and_open_minded_informed_but', 'open_minded_informed_but_not', 'minded_informed_but_not_limited', 'informed_but_not_limited_by', 'but_not_limited_by_your', 'not_limited_by_your_domain', 'limited_by_your_domain_of', 'by_your_domain_of_expertise']\n"
     ]
    }
   ],
   "source": [
    "n_gram_ranges = [2, 3, 4, 5]\n",
    "\n",
    "def format(tup):\n",
    "  return '_'.join(tup)\n",
    "\n",
    "for document in documents:\n",
    "  document['step_4'] = []\n",
    "  for tokens in document['step_3']:\n",
    "    sentence_grams = []\n",
    "    for n in n_gram_ranges:\n",
    "      sentence_grams.extend([ format(tup) for tup in nltk.ngrams(tokens, n)])\n",
    "    \n",
    "    document['step_4'].append(sentence_grams)\n",
    "\n",
    "print(documents[0]['step_3'][0])\n",
    "print()\n",
    "print(documents[0]['step_4'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you_are', 'are_passionate', 'passionate_about', 'about_your', 'your_area', 'area_of', 'of_expertise', 'expertise_deeply', 'deeply_inquisitive', 'inquisitive_and', 'and_open', 'open_minded', 'minded_informed', 'informed_but', 'but_not', 'not_limited', 'limited_by', 'by_your', 'your_domain', 'domain_of', 'of_expertise', 'you_are_passionate', 'are_passionate_about', 'passionate_about_your', 'about_your_area', 'your_area_of', 'area_of_expertise', 'of_expertise_deeply', 'expertise_deeply_inquisitive', 'deeply_inquisitive_and', 'inquisitive_and_open', 'and_open_minded', 'open_minded_informed', 'minded_informed_but', 'informed_but_not', 'but_not_limited', 'not_limited_by', 'limited_by_your', 'by_your_domain', 'your_domain_of', 'domain_of_expertise', 'you_are_passionate_about', 'are_passionate_about_your', 'passionate_about_your_area', 'about_your_area_of', 'your_area_of_expertise', 'area_of_expertise_deeply', 'of_expertise_deeply_inquisitive', 'expertise_deeply_inquisitive_and', 'deeply_inquisitive_and_open', 'inquisitive_and_open_minded', 'and_open_minded_informed', 'open_minded_informed_but', 'minded_informed_but_not', 'informed_but_not_limited', 'but_not_limited_by', 'not_limited_by_your', 'limited_by_your_domain', 'by_your_domain_of', 'your_domain_of_expertise', 'you_are_passionate_about_your', 'are_passionate_about_your_area', 'passionate_about_your_area_of', 'about_your_area_of_expertise', 'your_area_of_expertise_deeply', 'area_of_expertise_deeply_inquisitive', 'of_expertise_deeply_inquisitive_and', 'expertise_deeply_inquisitive_and_open', 'deeply_inquisitive_and_open_minded', 'inquisitive_and_open_minded_informed', 'and_open_minded_informed_but', 'open_minded_informed_but_not', 'minded_informed_but_not_limited', 'informed_but_not_limited_by', 'but_not_limited_by_your', 'not_limited_by_your_domain', 'limited_by_your_domain_of', 'by_your_domain_of_expertise']\n",
      "\n",
      "850\n"
     ]
    }
   ],
   "source": [
    "for document in documents:\n",
    "  document['step_5'] = []\n",
    "  for tokens in document['step_4']:\n",
    "    document['step_5'].extend(tokens)\n",
    "  \n",
    "print(documents[0]['step_4'][0])\n",
    "print()\n",
    "print(len(documents[0]['step_5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ability_to', 264), ('experience_in', 128), ('in_a', 110), ('knowledge_of', 98), ('experience_with', 96), ('to_work', 94), ('understanding_of', 92), ('communication_skills', 67), ('you_have', 65), ('years_of', 65), ('able_to', 64), ('familiarity_with', 59), ('you_are', 51), ('in_the', 49), ('with_a', 48), ('skills_and', 45), ('experience_working', 44), ('the_ability', 41), ('the_ability_to', 41), ('such_as', 38)]\n"
     ]
    }
   ],
   "source": [
    "qualifications = [ document['step_5'] for document in documents ]\n",
    "\n",
    "## switch to tf-idf ... build up stop words...\n",
    "\n",
    "bag_words = Counter(itertools.chain(*qualifications))\n",
    "print(bag_words.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  text = re.sub(r'e\\.g\\.', '', text)\n",
    "  text = re.sub(r'etc\\.', 'etc', text)\n",
    "  text = re.sub(r\"'\", '', text)\n",
    "\n",
    "  text = re.sub(r'[*•\"]', ' ', text)\n",
    "  text = re.sub(r'\\/', ' or ', text)\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "  text = text.lower()\n",
    "\n",
    "  return sent_tokenize(text.strip())\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "more_stop_words = [\n",
    "  '.',\n",
    "  ';',\n",
    "  ')',\n",
    "  '(',\n",
    "  ',',\n",
    "  ':',\n",
    "  '&',\n",
    "  '•',\n",
    "  \"’\",\n",
    "  '-',\n",
    "]\n",
    "\n",
    "bad_words = [\n",
    "  'ability',\n",
    "  'experience',\n",
    "  'knowledge',\n",
    "  'proven',\n",
    "  'skill',\n",
    "  'year',\n",
    "  'disney',\n",
    "  'strong'\n",
    "]\n",
    "\n",
    "stop_words.extend(bad_words)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "qualifications = []\n",
    "for description in descriptions:\n",
    "  items = []\n",
    "  for item in description['sections']:\n",
    "    if item['section'] == 'Basic Qualifications' or item['section'] == 'Preferred Qualifications':\n",
    "      for sentence in clean_text(item['text']):\n",
    "        tokens = [tk for tk in word_tokenize(sentence) if re.search('\\w', tk) != None]\n",
    "        tokens = [tk for tk in word_tokenize(sentence) if not tk.isdigit()]\n",
    "        tokens = [lemmatizer.lemmatize(tk) for tk in tokens]\n",
    "        tokens = [tk for tk in tokens if tk not in more_stop_words]\n",
    "\n",
    "        items.extend([ f'{t1}_{t2}' for t1, t2 in nltk.ngrams(tokens, n=2) if not (t1 in stop_words or t2 in stop_words)])\n",
    "\n",
    "  if len(items) == 0:\n",
    "    continue\n",
    "\n",
    "  qualification = {\n",
    "    'cat_id': description['cat_id'],\n",
    "    'job_id': description['job_id'],\n",
    "    'document': items\n",
    "  }\n",
    "\n",
    "  qualifications.append(qualification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['expertise_deeply',\n",
       " 'deeply_inquisitive',\n",
       " 'open_minded',\n",
       " 'minded_informed',\n",
       " 'enjoy_intellectual']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [ item['document'] for item in qualifications ]\n",
    "documents[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('project_management', 24), ('team_player', 21), ('verbal_communication', 18), ('team_environment', 18), ('microsoft_office', 17), ('per_week', 17), ('night_weekend', 16), ('related_field', 15), ('graphic_design', 14), ('adobe_creative', 13), ('creative_suite', 13), ('written_communication', 13), ('handle_multiple', 13), ('including_night', 13), ('full_availability', 13), ('photoshop_illustrator', 12), ('manage_multiple', 12), ('cast_member', 12), ('day_per', 11), ('team_member', 10)]\n"
     ]
    }
   ],
   "source": [
    "bag_words = Counter(itertools.chain(*documents))\n",
    "print(bag_words.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0'\n",
      "  '0.002*\"best_practice\" + 0.002*\"theme_park\" + 0.002*\"team_player\" + 0.002*\"team_environment\" + 0.002*\"project_management\" + 0.002*\"work_ethic\" + 0.002*\"scripting_language\" + 0.002*\"handle_multiple\" + 0.002*\"scheduling_method\" + 0.002*\"written_communication\"']\n",
      " ['1'\n",
      "  '0.004*\"night_weekend\" + 0.004*\"full_availability\" + 0.004*\"including_night\" + 0.003*\"per_week\" + 0.003*\"day_per\" + 0.003*\"guest_service\" + 0.002*\"microsoft_office\" + 0.002*\"work_well\" + 0.002*\"verbal_communication\" + 0.002*\"week_including\"']\n",
      " ['2'\n",
      "  '0.003*\"project_management\" + 0.003*\"related_field\" + 0.002*\"graphic_design\" + 0.002*\"computer_science\" + 0.002*\"verbal_communication\" + 0.002*\"programming_language\" + 0.002*\"mathematics_statistic\" + 0.002*\"economics_engineering\" + 0.002*\"statistic_economics\" + 0.002*\"environmental_graphic\"']\n",
      " ['3'\n",
      "  '0.002*\"machine_learning\" + 0.002*\"microsoft_office\" + 0.002*\"adobe_creative\" + 0.002*\"team_player\" + 0.002*\"photoshop_illustrator\" + 0.002*\"data_management\" + 0.002*\"mechanical_system\" + 0.002*\"creative_suite\" + 0.001*\"project_management\" + 0.001*\"team_environment\"']\n",
      " ['4'\n",
      "  '0.002*\"cast_member\" + 0.002*\"fast-paced_environment\" + 0.002*\"product_management\" + 0.002*\"good_communication\" + 0.002*\"photoshop_illustrator\" + 0.002*\"per_week\" + 0.002*\"creative_suite\" + 0.002*\"industry_trend\" + 0.002*\"handle_multiple\" + 0.002*\"adobe_creative\"']]\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(documents)\n",
    "document_term_matrix = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "model = LdaModel(\n",
    "  document_term_matrix,\n",
    "  num_topics=5,\n",
    "  id2word=dictionary,\n",
    "  passes=10,\n",
    ")\n",
    "\n",
    "print(\n",
    "  np.array(model.print_topics(num_topics=5, num_words=10))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19757793040 (3, 0.97833526)\n",
      "26800277248 (3, 0.9897175)\n",
      "25793337456 (4, 0.97315794)\n",
      "26601275040 (0, 0.9886906)\n",
      "24704474768 (2, 0.98882234)\n",
      "24704474480 (2, 0.9842789)\n",
      "24429981920 (3, 0.9902122)\n",
      "20042573200 (3, 0.9740526)\n",
      "27441961376 (3, 0.9702436)\n",
      "26304165456 (2, 0.9888663)\n",
      "24259248272 (0, 0.0175085) (1, 0.9320049) (2, 0.016907346) (3, 0.01677804) (4, 0.01680117)\n",
      "27629588832 (4, 0.989157)\n",
      "27272804544 (2, 0.974919)\n",
      "27266426816 (2, 0.9787574)\n",
      "27506594224 (0, 0.9894405)\n",
      "26644660320 (3, 0.9664998)\n",
      "27699308208 (1, 0.9864001)\n",
      "25947875248 (2, 0.9821542)\n",
      "27155015808 (0, 0.9857028)\n",
      "24573292192 (0, 0.015492004) (1, 0.015553833) (2, 0.01543027) (3, 0.015502573) (4, 0.93802136)\n",
      "24200527168 (2, 0.9894666)\n",
      "24170652720 (2, 0.9808334)\n",
      "23025374752 (3, 0.9722748)\n",
      "13560672608 (0, 0.978839)\n",
      "27088782608 (0, 0.01003276) (1, 0.010044117) (2, 0.010034749) (3, 0.010025941) (4, 0.9598625)\n",
      "27077496880 (2, 0.98042506)\n",
      "27032698496 (0, 0.97706246)\n",
      "26606093104 (0, 0.010032748) (1, 0.0100442385) (2, 0.010034758) (3, 0.010025945) (4, 0.9598623)\n",
      "26435839936 (1, 0.98353446)\n",
      "26435816656 (0, 0.011229635) (1, 0.011260024) (2, 0.011227175) (3, 0.011213416) (4, 0.9550697)\n",
      "26421625344 (4, 0.97769296)\n",
      "26421612896 (2, 0.9665483)\n",
      "25804673536 (0, 0.97830385)\n",
      "25554435232 (2, 0.96790147)\n",
      "25131649216 (0, 0.98707914)\n",
      "25131645584 (0, 0.98332417)\n",
      "24641873728 (2, 0.9616986)\n",
      "24259253248 (4, 0.9933116)\n",
      "24200527440 (0, 0.016712585) (1, 0.016675042) (2, 0.016688708) (3, 0.93315715) (4, 0.01676653)\n",
      "23870703344 (1, 0.97827315)\n",
      "23333465824 (2, 0.98283696)\n",
      "21407266064 (2, 0.9859283)\n",
      "17617622720 (2, 0.98360014)\n",
      "7255959040 (3, 0.9770098)\n",
      "25793359904 (0, 0.9422419) (1, 0.014416916) (2, 0.014495651) (3, 0.01435418) (4, 0.014491355)\n",
      "27457761616 (0, 0.015424732) (1, 0.015391904) (2, 0.015406965) (3, 0.015392202) (4, 0.9383842)\n",
      "26779119872 (3, 0.9663726)\n",
      "25922527760 (2, 0.97135913)\n",
      "23329326352 (2, 0.97018325)\n",
      "23183053712 (1, 0.97889215)\n",
      "16234915008 (0, 0.050027393) (1, 0.050026685) (2, 0.050026137) (3, 0.7998921) (4, 0.050027687)\n",
      "27611761792 (0, 0.9883648)\n",
      "26829879952 (3, 0.9890159)\n",
      "26724244240 (1, 0.9712963)\n",
      "26216960336 (1, 0.99221617)\n",
      "25351505360 (1, 0.98212224)\n",
      "24636972224 (4, 0.97888213)\n",
      "24636944096 (2, 0.9863826)\n",
      "24588543440 (3, 0.9872897)\n",
      "21286977968 (2, 0.9883956)\n",
      "27550252256 (1, 0.98128027)\n",
      "27506594064 (0, 0.9832853)\n",
      "27200312192 (0, 0.01342943) (1, 0.94638973) (2, 0.013435217) (3, 0.013366975) (4, 0.013378624)\n",
      "27150046384 (1, 0.9892993)\n",
      "27082386416 (2, 0.97631174)\n",
      "26779112624 (1, 0.97317797)\n",
      "26761268944 (4, 0.9804773)\n",
      "26637704064 (1, 0.99011546)\n",
      "26637703936 (1, 0.98946595)\n",
      "26415117168 (1, 0.98329127)\n",
      "26288058352 (2, 0.9691187)\n",
      "26229767216 (3, 0.98742235)\n",
      "25854477200 (1, 0.9781734)\n",
      "25747556960 (1, 0.97567487)\n",
      "25747556816 (1, 0.97491264)\n",
      "25747556656 (1, 0.9749127)\n",
      "25137368128 (0, 0.011770134) (1, 0.011862838) (2, 0.95273745) (3, 0.011803234) (4, 0.011826359)\n",
      "25137360720 (0, 0.011770132) (1, 0.01186396) (2, 0.9527363) (3, 0.011803235) (4, 0.011826376)\n",
      "25001098864 (2, 0.98544353)\n",
      "24864114304 (0, 0.98137033)\n",
      "23892498304 (4, 0.98163813)\n",
      "20892574592 (0, 0.98137033)\n",
      "16360548592 (2, 0.96174085)\n",
      "5726106000 (3, 0.9786771)\n",
      "27855101056 (0, 0.010632587) (1, 0.010556025) (2, 0.9577212) (3, 0.010537991) (4, 0.010552179)\n",
      "27851186288 (3, 0.9809431)\n",
      "27851185408 (3, 0.9809431)\n",
      "27662461456 (4, 0.9756592)\n",
      "27635838304 (3, 0.9870524)\n",
      "27629594304 (0, 0.9832705)\n",
      "27625241008 (4, 0.98039454)\n",
      "27579278544 (3, 0.97551847)\n",
      "27559459632 (2, 0.97411776)\n",
      "27550252288 (2, 0.981363)\n",
      "27517914032 (3, 0.9679453)\n",
      "27506595200 (2, 0.9793893)\n",
      "27446648784 (4, 0.9841075)\n",
      "27266426752 (4, 0.96896195)\n",
      "27211199744 (0, 0.9825448)\n",
      "27168034960 (4, 0.9859129)\n",
      "27103436176 (4, 0.974881)\n",
      "26688731728 (0, 0.95545226) (1, 0.0111383) (2, 0.011142175) (3, 0.011131058) (4, 0.011136242)\n",
      "26387015248 (2, 0.9617605)\n",
      "26156952000 (0, 0.9792791)\n",
      "26156951968 (4, 0.980396)\n",
      "25854469776 (2, 0.9768219)\n",
      "25561189520 (1, 0.97234696)\n",
      "25561188240 (1, 0.9723467)\n",
      "25435107648 (2, 0.989468)\n",
      "25137365136 (3, 0.9689031)\n",
      "25013171808 (2, 0.984262)\n",
      "25013167568 (2, 0.98659176)\n",
      "25001098880 (2, 0.9890352)\n",
      "24573302128 (4, 0.97411025)\n",
      "24124709104 (0, 0.98836124)\n",
      "21392955472 (2, 0.98536533)\n",
      "20429183952 (0, 0.010548717) (1, 0.9578214) (2, 0.010555659) (3, 0.010537166) (4, 0.010536977)\n",
      "20143592448 (2, 0.97888553)\n",
      "582588672 (4, 0.98457974)\n",
      "4534138000 (1, 0.9649979)\n",
      "4534137840 (0, 0.011771588) (1, 0.011866952) (2, 0.011772847) (3, 0.011860387) (4, 0.9527283)\n",
      "4534137040 (3, 0.9688853)\n",
      "27780781936 (3, 0.9898503)\n",
      "27579281248 (0, 0.9894326)\n",
      "27478060016 (4, 0.9748023)\n",
      "27478059968 (4, 0.9679645)\n",
      "27478059952 (0, 0.9427568) (1, 0.014291005) (2, 0.014340367) (3, 0.014291238) (4, 0.014320609)\n",
      "27478059936 (0, 0.9594975) (1, 0.01004675) (2, 0.01002786) (3, 0.010081936) (4, 0.010345921)\n",
      "27225844592 (4, 0.9872736)\n",
      "27225844352 (4, 0.9799221)\n",
      "27155014240 (1, 0.98594457)\n",
      "27155014208 (1, 0.98594457)\n",
      "27064544304 (1, 0.990453)\n",
      "26992390832 (0, 0.98418236)\n",
      "26713166704 (3, 0.9870467)\n",
      "26699601072 (1, 0.9956454)\n",
      "26688732128 (3, 0.9886799)\n",
      "26469543424 (1, 0.9885489)\n",
      "26469540640 (1, 0.9894458)\n",
      "26439104800 (0, 0.9923675)\n",
      "26200510976 (4, 0.9776291)\n",
      "26192258656 (2, 0.98589826)\n",
      "26139685376 (1, 0.9770233)\n",
      "24636969824 (3, 0.9804205)\n",
      "24573315312 (4, 0.988204)\n",
      "24525310416 (1, 0.9770704)\n",
      "24184282736 (4, 0.9839683)\n",
      "23086003744 (1, 0.99302983)\n",
      "22166953472 (0, 0.9865823)\n",
      "1803495712 (0, 0.015472372) (1, 0.015804203) (2, 0.93783826) (3, 0.015434776) (4, 0.015450371)\n",
      "17880405088 (0, 0.9882172)\n",
      "17501995776 (2, 0.99009323)\n",
      "27716762704 (0, 0.98986024)\n",
      "27550252720 (4, 0.9908944)\n",
      "24214078320 (2, 0.9829472)\n",
      "12128248944 (4, 0.9762898)\n",
      "27873373936 (2, 0.9616701)\n",
      "27861261296 (4, 0.9757098)\n",
      "27868816304 (4, 0.98131317)\n",
      "27873373664 (4, 0.97567517)\n",
      "27880153024 (0, 0.9740786)\n",
      "27873375440 (0, 0.9817353)\n",
      "27930209104 (2, 0.98686045)\n",
      "27934916224 (3, 0.97643286)\n",
      "27930208640 (2, 0.9740601)\n",
      "27991931456 (3, 0.98487055)\n",
      "27991924976 (0, 0.95980436) (1, 0.0100058485) (2, 0.010009089) (3, 0.010006233) (4, 0.010174483)\n",
      "27996730304 (3, 0.988182)\n",
      "28013695616 (3, 0.9853808)\n",
      "28018869648 (0, 0.9817596)\n"
     ]
    }
   ],
   "source": [
    "for document in qualifications:\n",
    "    job_id = document['job_id']\n",
    "    bow = dictionary.doc2bow(document['document'])\n",
    "    topic = model.get_document_topics(bow)\n",
    "\n",
    "    print(job_id, *topic)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2304a4306d72a9fc8f52cfa19ebef60f8a2cea47ec9230cdf955b20fb7741e1a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('monte': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
